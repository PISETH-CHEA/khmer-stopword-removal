{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 02: Stopword Identification Process\n",
    "# Objective: Demonstrate frequency analysis and linguistic rule application for stopword detection.\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.stopword_detection.frequency_analyzer import FrequencyAnalyzer\n",
    "from src.stopword_detection.linguistic_rules import LinguisticRules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c1169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components with correct config path\n",
    "freq_analyzer = FrequencyAnalyzer(config_path=\"../config/config.yaml\")\n",
    "ling_rules = LinguisticRules()\n",
    "\n",
    "segmented_dir = \"../data/segmented\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== STOPWORD IDENTIFICATION PIPELINE ===\")\n",
    "# Load configuration\n",
    "with open(\"../config/config.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Frequency threshold: {config['stopword_detection']['frequency_threshold']}\")\n",
    "print(f\"Min document frequency: {config['stopword_detection']['min_doc_frequency']}\")\n",
    "\n",
    "# Frequency Analysis\n",
    "print(\"\\nRunning frequency analysis...\")\n",
    "freq_candidates = freq_analyzer.analyze_corpus(segmented_dir)\n",
    "\n",
    "print(f\"Found {len(freq_candidates)} frequency-based candidates\")\n",
    "print(\"\\n=== TOP 15 FREQUENCY CANDIDATES ===\")\n",
    "\n",
    "freq_df = pd.DataFrame(freq_candidates[:15])\n",
    "print(freq_df[['word', 'doc_frequency_ratio', 'total_count']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3abecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot document frequency distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist([c['doc_frequency_ratio'] for c in freq_candidates], bins=20, edgecolor='black')\n",
    "plt.title('Distribution of Document Frequency Ratios')\n",
    "plt.xlabel('Document Frequency Ratio')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.axvline(x=config['stopword_detection']['frequency_threshold'], \n",
    "            color='red', linestyle='--', label=f\"Threshold ({config['stopword_detection']['frequency_threshold']})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linguistic Rules Analysis\n",
    "print(\"Building vocabulary for linguistic analysis...\")\n",
    "vocabulary = ling_rules.build_vocabulary(segmented_dir)\n",
    "print(f\"Total vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "print(\"\\nApplying linguistic rules...\")\n",
    "ling_candidates = ling_rules.identify_linguistic_stopwords(vocabulary)\n",
    "print(f\"Found {len(ling_candidates)} linguistic candidates\")\n",
    "\n",
    "print(\"\\n=== LINGUISTIC CANDIDATES ===\")\n",
    "print(', '.join(sorted(ling_candidates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64af2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize by grammatical function\n",
    "pronouns = ['ខ្ញុំ', 'អ្នក', 'គាត់', 'យើង', 'ពួកគេ', 'វា']\n",
    "prepositions = ['នៅ', 'ក្នុង', 'លើ', 'ក្រោម', 'ខាង', 'ជិត', 'ឆ្ងាយ']\n",
    "conjunctions = ['និង', 'ឬ', 'ប៉ុន្តែ', 'ដូច្នេះ', 'ព្រោះ', 'ទោះបី']\n",
    "particles = ['ជា', 'ដែល', 'អំពី', 'ដោយ', 'សម្រាប់', 'ពី']\n",
    "\n",
    "categories = {\n",
    "    'Pronouns': [w for w in ling_candidates if w in pronouns],\n",
    "    'Prepositions': [w for w in ling_candidates if w in prepositions],\n",
    "    'Conjunctions': [w for w in ling_candidates if w in conjunctions],\n",
    "    'Particles': [w for w in ling_candidates if w in particles]\n",
    "}\n",
    "\n",
    "for category, words in categories.items():\n",
    "    if words:\n",
    "        print(f\"\\n{category}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and analyze overlap\n",
    "freq_words = set([c['word'] for c in freq_candidates])\n",
    "ling_words = set(ling_candidates)\n",
    "\n",
    "overlap = freq_words.intersection(ling_words)\n",
    "only_freq = freq_words - ling_words\n",
    "only_ling = ling_words - freq_words\n",
    "\n",
    "print(f\"=== CANDIDATE OVERLAP ANALYSIS ===\")\n",
    "print(f\"Words in both lists: {len(overlap)} ({', '.join(list(overlap)[:10])})\")\n",
    "print(f\"Only frequency-based: {len(only_freq)}\")\n",
    "print(f\"Only linguistic: {len(only_ling)} ({', '.join(list(only_ling))})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ae863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final stopword list statistics\n",
    "final_stopwords = list(freq_words.union(ling_words))\n",
    "print(f\"\\n=== FINAL STOPWORD LIST ===\")\n",
    "print(f\"Total stopwords: {len(final_stopwords)}\")\n",
    "print(f\"Coverage: {len(final_stopwords)/len(vocabulary)*100:.1f}% of vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aad7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final list\n",
    "with open(\"../data/stopwords/final_stopword_list.txt\", 'w', encoding='utf-8') as f:\n",
    "    for word in sorted(final_stopwords):\n",
    "        f.write(f\"{word}\\n\")\n",
    "print(\"Final stopword list saved!\")\n",
    "\n",
    "print(\"\\n## Key Insights\")\n",
    "print(\"\\n1. **Frequency Threshold**: 0.3 (30% document frequency) captured 312 candidates\")\n",
    "print(\"2. **Linguistic Coverage**: 36 grammatical function words identified\")\n",
    "print(\"3. **Overlap**: Some words appear in both lists (strong stopword indicators)\")\n",
    "print(\"4. **Unique Contributions**: Frequency and linguistic methods complement each other\")\n",
    "print(\"5. **Final Coverage**: 319 stopwords covering ~18% of vocabulary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WIR (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
